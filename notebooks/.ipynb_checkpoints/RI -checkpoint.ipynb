{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Componentes da RI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statments\n",
    "import numpy\n",
    "import re\n",
    "\n",
    "'''\n",
    "Tokenize each the sentences, example\n",
    "Input : \"John likes to watch movies. Mary likes movies too\"\n",
    "Ouput : \"John\",\"likes\",\"to\",\"watch\",\"movies\",\"Mary\",\"likes\",\"movies\",\"too\"\n",
    "'''\n",
    "def tokenize(sentences):\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        w = word_extraction(sentence)\n",
    "        words.extend(w)\n",
    "        \n",
    "    words = sorted(list(set(words)))\n",
    "    return words\n",
    "\n",
    "def word_extraction(sentence):\n",
    "    ignore = ['a', \"the\", \"is\"]\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split()\n",
    "    cleaned_text = [w.lower() for w in words if w not in ignore]\n",
    "    return cleaned_text    \n",
    "    \n",
    "def generate_bow(allsentences,vocab):    \n",
    "    \n",
    "    print(\"Word List for Document \\n{0} \\n\".format(vocab));\n",
    "    a = numpy.zeros(shape=(len(allsentences),len(vocab)))\n",
    "    sentence_number=0\n",
    "    for sentence in allsentences:\n",
    "        words = word_extraction(sentence)\n",
    "        #bag_vector = numpy.zeros(len(vocab))\n",
    "        \n",
    "        for w in words:\n",
    "            for i,word in enumerate(vocab):\n",
    "                \n",
    "                if word == w: \n",
    "                    a[sentence_number][i] += 1\n",
    "       # a.append(bag_vector.tolist())            \n",
    "        sentence_number+=1\n",
    "        #print(\"{0} \\n{1}\\n\".format(sentence,numpy.array(a)))\n",
    "        \n",
    "    return(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word List for Document \n",
      "['amo', 'as', 'aulas', 'de', 'disciplina', 'em', 'eu', 'melhor', 'ri', 'todas', 'vir', 'vou', 'é'] \n",
      "\n",
      "[[0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]]\n",
      "Word List for Document \n",
      "['amo', 'as', 'aulas', 'de', 'disciplina', 'em', 'eu', 'melhor', 'ri', 'todas', 'vir', 'vou', 'é'] \n",
      "\n",
      "[[1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, False, False, False,  True, False, False,\n",
       "        False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False],\n",
       "       [ True, False, False, False, False, False,  True, False, False,\n",
       "        False, False, False, False]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def match_query(matrix, q,vocab):\n",
    "    #q = tokenize(q)\n",
    "    #print(q)\n",
    "    matrix_query=(generate_bow(q,vocab))\n",
    "   # print(matrix_query)\n",
    "    #print(matrix_query(numpy.nonzero(matrix_query)))\n",
    "    return matrix_query\n",
    "    \n",
    "    \n",
    "allsentences = [\"eu vou vir em todas as aulas de RI\", \n",
    "            \"RI  é a melhor disciplina \", \n",
    "            \"Eu amo RI\"]\n",
    "q=[\"amo eu\"]\n",
    "\n",
    "vocab = tokenize(allsentences)\n",
    "matrix=generate_bow(allsentences,vocab)\n",
    "print(matrix)\n",
    "q=match_query(matrix,q,vocab)\n",
    "print(q)\n",
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2)) \n",
    "\n",
    "#print(numpy.intersect1d(matrix, q))\n",
    "#numpy.intersect1d(matrix[0,:], q)\n",
    "\n",
    "numpy.logical_and(matrix, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m50\u001b[0m\n\u001b[0;31m    print(index)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "class BooleanModel(object):\n",
    "\tdef tokenize(self, text):\n",
    "\t\t\"\"\"Separa `text` numa lista de tokens, utilizando como delimitadores ',', '.', '!', '?' e ' '.\"\"\"\n",
    "\t\treturn [tokens.strip().replace(',', ' ').replace('.', ' ').replace('!', ' ').replace('?', ' ').split() for tokens in text]\n",
    "\n",
    "\tdef normalize(self, tokens, stopwords):\n",
    "\t\t\"\"\"Remove `stopwords` da lista `tokens`.\"\"\"\n",
    "\t\tnorm = []\n",
    "\t\tfor phrase in tokens:\n",
    "\t\t\tnorm.append([token.lower() for token in phrase if token.lower() not in stopwords])\n",
    "\n",
    "\t\treturn np.unique(np.hstack(np.array(norm)))\n",
    "\n",
    "\tdef create_index(self, tokens, docs):\n",
    "\t\t\"\"\"Cria matriz de incidência, a partir dos `docs` e `tokens` informados.\"\"\"\n",
    "\n",
    "\t\tfreq_matrix = {}\n",
    "\t\tfor token in tokens:\n",
    "\t\t\tfreq_matrix.update({token : np.char.count([doc.lower() for doc in docs], token)})\n",
    "\n",
    "\t\treturn (freq_matrix)\n",
    "\t\t\n",
    "\tdef query(self, q, matrix, stopwords, tokens, docs):\n",
    "\t\t\"\"\"Realiza consulta `q`, utilizando o modelo booleano para retornar os `docs` relevantes.\"\"\"\n",
    "\n",
    "\t\tq = self.normalize(self.tokenize(q), stopwords)\n",
    "\t\tor_result = set()\n",
    "\t\tand_result = set()\n",
    "\n",
    "\t\tdoc_index = {}\n",
    "\t\tfor option in q:\n",
    "\t\t\tdoc_index.update({option : matrix.get(option, np.array([0*len(docs)])).nonzero()})\n",
    "\t\t\t\n",
    "\t\t\t#operação OR\n",
    "\t\t\tfor index in doc_index[option]:\n",
    "\t\t\t\tprint(type(index))\n",
    "\t\t\t\tfor i in range(len(index)):\n",
    "\t\t\t\t\tor_result.add(docs[index[i]])\n",
    "\n",
    "\t\tand_index = doc_index[q[0]][0] if len(q) > 0 else ''\n",
    "\t\tfor i in range(len(q)-1):\n",
    "\t\t\tand_index = self.__intersect(and_index, doc_index[q[i+1]][0])\n",
    "\n",
    "\t\tfor index in and_index:\n",
    "\t\t\tprint(index)\n",
    "\t\t\tfor i in range(len(index)):\n",
    "\t\t\t\tand_result.add(docs[index[i]])\n",
    "\n",
    "\t\treturn (and_result, or_result)\n",
    "\n",
    "\tdef __intersect(self, q1, q2):\n",
    "\t\t\"\"\"Faz a intersecção de dois índices de documentos, `q1` e `q2` (operação AND)\"\"\"\n",
    "\t\tanswer = []\n",
    "\t\ti = 0\n",
    "\t\tj = 0\n",
    "\n",
    "\t\twhile len(q1) > i and len(q2) > j:\n",
    "\t\t\tif q1[i] == q2[j]:\n",
    "\t\t\t\tanswer.append(q1[i])\n",
    "\t\t\t\ti += 1\n",
    "\t\t\t\tj += 1\n",
    "\t\t\telif q1[i] < q2[j]:\n",
    "\t\t\t\ti += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tj += 1\n",
    "\n",
    "\t\treturn answer\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tbm = BooleanModel()\n",
    "\ttext = ['xadrez','teste', 'xadrez teste','lixo']\n",
    "\tstopwords = ['a', 'o', 'e', 'é', 'de', 'do', 'no', 'são']\n",
    "\tq = ['xadrez teste']\n",
    "\n",
    "\ttokens = bm.normalize(bm.tokenize(text), stopwords)\n",
    "\tfm = bm.create_index(tokens, text)\n",
    "\tresult = bm.query(q, fm, stopwords, tokens, text)\n",
    "\tprint(\"result  \",result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
